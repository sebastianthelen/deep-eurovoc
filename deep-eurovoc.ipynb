{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepEurovoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this little experiment is to predict Eurovoc categories/fields of publications based on expression abstracts published by the PO. \n",
    "\n",
    "The model used in this notebook is based on the 1D CNN described in the Keras blog article https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html. It uses pre-trained GloVe word embeddings for text classification.\n",
    "\n",
    "The input data is retrieved from the public SPARQL endpoint of the PO which is available at http://publications.europa.eu/webapi/rdf/sparql. The following query does the job:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "prefix cdm: <http://publications.europa.eu/ontology/cdm#> \n",
    "select ?exp ?abstract (group_concat(?concept;separator=\";\") as ?concepts)  where \n",
    "{\n",
    "\t?exp cdm:expression_uses_language <http://publications.europa.eu/resource/authority/language/ENG>.\n",
    "\t?exp cdm:expression_abstract ?abstract.\n",
    "\t?exp cdm:expression_belongs_to_work/cdm:work_is_about_concept_eurovoc ?concept.\n",
    "} group by ?exp ?abstract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data has already been downloaded and is stored in data.csv. Before loading the data, the modules needed throughout this notebook are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as tfb\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of some global variables used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANUP_DATA = True # save time by loading a cleaned up version from disc\n",
    "MAX_NUM_WORDS = 20000 # max. size of vocabulary\n",
    "EMBEDDING_DIM = 100 # dimension of GloVe word embeddings\n",
    "MAX_SEQUENCE_LENGTH = 1000 # truncate examples after MAX_SEQUENCE_LENGTH words\n",
    "VALIDATION_SPLIT = 0.2 # ration for split of training data and test data\n",
    "NUM_EPOCHS = 10 # number of epochs the network is trained\n",
    "# those are the eurovoc fields we want to support\n",
    "EUROVOC_FIELDS = {\n",
    "\"4\":  \"POLITICS\",\n",
    "\"8\":  \"INTERNATIONAL RELATIONS\", \n",
    "\"10\": \"EUROPEAN UNION\",\n",
    "\"12\": \"LAW \",\n",
    "\"16\": \"ECONOMICS\",\n",
    "\"20\": \"TRADE\",\n",
    "\"24\": \"FINANCE\",\n",
    "\"28\": \"SOCIAL QUESTIONS\",\n",
    "\"32\": \"EDUCATION AND COMMUNICATIONS\",\n",
    "\"36\": \"SCIENCE\",\n",
    "\"40\": \"BUSINESS AND COMPETITION\",\n",
    "\"44\": \"EMPLOYMENT AND WORKING CONDITIONS\",\n",
    "\"48\": \"TRANSPORT\",\n",
    "\"52\": \"ENVIRONMENT\",\n",
    "\"56\": \"AGRICULTURE, FORESTRY AND FISHERIES\",\n",
    "\"60\": \"AGRI-FOODSTUFFS\",\n",
    "\"64\": \"PRODUCTION, TECHNOLOGY AND RESEARCH\",\n",
    "\"66\": \"ENERGY\",\n",
    "\"68\": \"INDUSTRY\",\n",
    "\"72\": \"GEOGRAPHY\",\n",
    "\"76\": \"INTERNATIONAL ORGANISATIONS\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and get some numbers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35472 entries, 0 to 35471\n",
      "Data columns (total 3 columns):\n",
      "exp         35472 non-null object\n",
      "abstract    35472 non-null object\n",
      "concepts    35472 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 831.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"data.csv\")\n",
    "print(data_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that will help us to extract the abstracts as plain text from the XMLLiterals returned by Virtuoso, to remove all non-English words including stop words, and to perform lemmatization on the filtered words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_abstract(xmlstring):\n",
    "    #import ipdb; ipdb.set_trace()\n",
    "    xmlstring = xmlstring.replace('\"\"', '\"')\n",
    "    text = None\n",
    "    try: \n",
    "        tree = ET.ElementTree(ET.fromstring(xmlstring))\n",
    "        xpath_result = tree.findall(\".//description\")\n",
    "        text = xpath_result[0].text\n",
    "    except:\n",
    "        text = xmlstring\n",
    "    # remove stopwords and punctuation. lower case everything\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w.lower() for w in tokens if not w in stop_words and w.isalpha() and wordnet.synsets(w)]\n",
    "    # lemmatize\n",
    "    lemma = WordNetLemmatizer()\n",
    "    final_tokens = []\n",
    "    for word in tokens:\n",
    "        final_tokens.append(lemma.lemmatize(word))\n",
    "    ret = \" \".join(final_tokens)\n",
    "    return ret\n",
    "    #return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data involves two steps:\n",
    "\n",
    "1\\. processing of the abstracts \n",
    "\n",
    "2\\. transform the \";\" separated eurovoc codes in the concepts column into lists of eurovoc categories. Only consider to first two digits of the eurovoc codes. The total number of codes  in the data set is much too high (>5000) for classification. Therefore, the first two digits indicating the \"subject matter\" are extracted and filtered based on EUROVOC_FIELDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if CLEANUP_DATA:\n",
    "    data_df[\"clean_abstract\"] = data_df[\"abstract\"].apply(cleanup_abstract)\n",
    "    data_df[\"clean_concepts\"] = data_df[\"concepts\"].apply(lambda x: list({c[c.rfind(\"/\")+1:c.rfind(\"/\")+3] for c in x.split(\";\") if c[c.rfind(\"/\")+1:c.rfind(\"/\")+3] in EUROVOC_FIELDS}))\n",
    "    data_df = data_df[data_df.astype(str)['clean_concepts'] != '[]']\n",
    "    data_df.drop([\"abstract\"], axis=1)\n",
    "    data_df.drop([\"concepts\"], axis=1)\n",
    "    data_df.to_pickle(\"data_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line loads the cleaned up data directly from file instead of computing it everytime the notebook is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_pickle(\"data_df.pkl\")\n",
    "print(data_df['clean_abstract'][:5])\n",
    "print(data_df['clean_concepts'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must convert the data set labels to numbers so that they can be processed by Keras. The approach is described in https://www.pyimagesearch.com/2018/05/07/multi-label-classification-with-keras/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 10\n",
      "2. 12\n",
      "3. 16\n",
      "4. 20\n",
      "5. 24\n",
      "6. 28\n",
      "7. 32\n",
      "8. 36\n",
      "9. 40\n",
      "10. 44\n",
      "11. 48\n",
      "12. 52\n",
      "13. 56\n",
      "14. 60\n",
      "15. 64\n",
      "16. 66\n",
      "17. 68\n",
      "18. 72\n",
      "19. 76\n",
      "20. 8\n"
     ]
    }
   ],
   "source": [
    "labels = data_df[\"clean_concepts\"].tolist()\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(labels)\n",
    "# loop over each of the possible class labels and show them\n",
    "for (i, label) in enumerate(mlb.classes_):\n",
    "\tprint(\"{}. {}\".format(i + 1, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small test to make sure that MultiLabelBinarizer is really generating multi-label vectors and not just one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels of the 2nd training example: [('24', '44', '52')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels of the 2nd training example: \" + str(mlb.inverse_transform(np.array([labels[1]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to tranform the input examples into an array of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianthelen/anaconda3/lib/python3.6/site-packages/keras/preprocessing/text.py:172: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76059 unique tokens.\n",
      "Shape of data tensor: (31589, 1000)\n",
      "Shape of label tensor: (31589, 20)\n"
     ]
    }
   ],
   "source": [
    "data = data_df[\"clean_abstract\"].tolist()\n",
    "tokenizer = Tokenizer(nb_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "#num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "#print(num_validation_samples)\n",
    "#trainX = data[:-num_validation_samples]\n",
    "#trainY = labels[:-num_validation_samples]\n",
    "#testX = data[-num_validation_samples:]\n",
    "#testY = labels[-num_validation_samples:]\n",
    "\n",
    "#print(\"trainX.shape\", trainX.shape)\n",
    "#print(\"trainY.shape\", trainY.shape)\n",
    "#print(\"testX.shape\", testX.shape)\n",
    "#print(\"testY.shape\", testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-computed GloVe word embeddings from file and create an embeddings_index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-2f0cbf6e876a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mcoefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0membeddings_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoefs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(os.path.join('glove.6B', 'glove.6B.100d.txt'), 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use word_index and embedding_index to compute the embedding_matrix. embedding_matrix is  a matrix storing the embedded_vector for each word in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    #else:\n",
    "    #    print(\"Not not in embedding index: \" + word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an Keras embedding_layer. Note that trainable=false, i.e., weights are not getting updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the 1D convolutional model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for POS_WEIGHT in [1, 5, 10]:\n",
    "    for DROPOUT in [0.1, 0.25, 0.5]:\n",
    "        for REGULARIZATION in [0.001, 0.01, 0.1]:\n",
    "            for BATCH_SIZE in [256]:\n",
    "                \n",
    "                params ={\"pos_weight\": str(POS_WEIGHT), \n",
    "                          \"dropout\": str(DROPOUT), \n",
    "                          \"regularization\": str(REGULARIZATION), \n",
    "                          \"batch_size\": str(BATCH_SIZE)}\n",
    "                \n",
    "                def weighted_binary_crossentropy(target, output):\n",
    "                    \"\"\"\n",
    "                    Weighted binary crossentropy between an output tensor \n",
    "                    and a target tensor. POS_WEIGHT is used as a multiplier \n",
    "                    for the positive targets.\n",
    "\n",
    "                    Combination of the following functions:\n",
    "                    * keras.losses.binary_crossentropy\n",
    "                    * keras.backend.tensorflow_backend.binary_crossentropy\n",
    "                    * tf.nn.weighted_cross_entropy_with_logits\n",
    "                    \"\"\"\n",
    "                    # transform back to logits\n",
    "                    _epsilon = tfb._to_tensor(tfb.epsilon(), output.dtype.base_dtype)\n",
    "                    output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
    "                    output = tf.log(output / (1 - output))\n",
    "                    # compute weighted loss\n",
    "                    loss = tf.nn.weighted_cross_entropy_with_logits(targets=target,\n",
    "                                                                    logits=output,\n",
    "                                                                    pos_weight=POS_WEIGHT)\n",
    "                    return tf.reduce_mean(loss, axis=-1)\n",
    "\n",
    "\n",
    "                sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "                embedded_sequences = embedding_layer(sequence_input)\n",
    "                x = Conv1D(128, 5, activation='relu', kernel_regularizer=regularizers.l2(REGULARIZATION))(embedded_sequences)\n",
    "                x = MaxPooling1D(5)(x)\n",
    "                x = Conv1D(128, 5, activation='relu', kernel_regularizer=regularizers.l2(REGULARIZATION))(x)\n",
    "                x = MaxPooling1D(5)(x)\n",
    "                x = Conv1D(128, 5, activation='relu', kernel_regularizer=regularizers.l2(REGULARIZATION))(x)\n",
    "                x = MaxPooling1D(35)(x)  # global max pooling\n",
    "                x = Flatten()(x)\n",
    "                x = Dropout(DROPOUT)(x)\n",
    "                x = Dense(128, activation='relu',kernel_regularizer=regularizers.l2(REGULARIZATION))(x)\n",
    "                x = Dropout(DROPOUT)(x)\n",
    "                preds = Dense(labels.shape[1], activation='sigmoid', kernel_regularizer=regularizers.l2(REGULARIZATION))(x)\n",
    "\n",
    "\n",
    "                import keras.losses\n",
    "                keras.losses.weighted_binary_crossentropy = weighted_binary_crossentropy\n",
    "\n",
    "                model = Model(sequence_input, preds)\n",
    "                model.compile(loss='weighted_binary_crossentropy',\n",
    "                              optimizer='adam',\n",
    "                              metrics=['categorical_accuracy'])\n",
    "\n",
    "                history = model.fit(data[:100], labels[:100], validation_split=0.2,\n",
    "                          epochs=NUM_EPOCHS, batch_size=BATCH_SIZE)\n",
    "                \n",
    "                model.save(\"models/model_%(pos_weight)s_%(dropout)s_%(regularization)s_%(batch_size)s.h5\"%params)\n",
    "                \n",
    "                acc = history.history['categorical_accuracy']\n",
    "                val_acc = history.history['val_categorical_accuracy']\n",
    "                loss = history.history['loss']\n",
    "                val_loss = history.history['val_loss']\n",
    "\n",
    "                epochs = range(len(acc))\n",
    "                \n",
    "                fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "                \n",
    "\n",
    "                ax1.plot(epochs, acc, label='Training acc')\n",
    "                ax1.plot(epochs, val_acc, label='Validation acc')\n",
    "                ax1.set_ylabel('accuracy')\n",
    "                ax1.set_xlabel('epoch')\n",
    "                ax1.set_title('Accuracy: PW %(pos_weight)s, DO %(dropout)s, REG %(regularization)s, BATCH %(batch_size)s'% params)\n",
    "                ax1.legend()\n",
    "                \n",
    "                #plt.figure()\n",
    "\n",
    "                ax2.plot(epochs, loss, label='Training loss')\n",
    "                ax2.plot(epochs, val_loss, label='Validation loss')\n",
    "                ax2.set_ylabel('loss')\n",
    "                ax2.set_xlabel('epoch')\n",
    "                ax2.set_title('Loss: PW %(pos_weight)s, DO %(dropout)s, REG %(regularization)s, BATCH %(batch_size)s'%params)\n",
    "                ax2.legend()\n",
    "                \n",
    "                \n",
    "                fig.savefig(\"figures/model_%(pos_weight)s_%(dropout)s_%(regularization)s_%(batch_size)s.png\"%params)\n",
    "                \n",
    "                plt.show()\n",
    "                #f.savefig(\"model_%(pos_weight)s_%(dropout)s_%(regularization)s_%(batch_size)s.png\"%params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model needs to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to disc!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict using an example from the training set. Later we should also create a dev set for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
