{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepEurovoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this experiment is to predict Eurovoc codes based on expression abstracts published by the PO. \n",
    "\n",
    "The model used in this notebook is identical to the 1-dimensional convolutional neural network described in the Keras blog article https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html. It uses pre-trained GloVe word embeddings for text classification.\n",
    "\n",
    "The input is retrieved from PO's public SPARQL endpoint, available at http://publications.europa.eu/webapi/rdf/sparql. The following query does the job:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "prefix cdm: <http://publications.europa.eu/ontology/cdm#> \n",
    "select ?exp ?abstract (group_concat(?concept;separator=\";\") as ?concepts)  where \n",
    "{\n",
    "\t?exp cdm:expression_uses_language <http://publications.europa.eu/resource/authority/language/ENG>.\n",
    "\t?exp cdm:expression_abstract ?abstract.\n",
    "\t?exp cdm:expression_belongs_to_work/cdm:work_is_about_concept_eurovoc ?concept.\n",
    "} group by ?exp ?abstract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data is stored in data.csv. Before loading the data, we import the modules we will need throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of some global variables used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLEANUP_DATA = True \n",
    "MAX_NUM_WORDS = 20000 # size of vocabulary\n",
    "EMBEDDING_DIM = 100 # dimension of GloVe word embeddings\n",
    "MAX_SEQUENCE_LENGTH = 1000 # truncate abstracts after MAX_SEQUENCE_LENGTH words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and get some numbers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35472 entries, 0 to 35471\n",
      "Data columns (total 3 columns):\n",
      "exp         35472 non-null object\n",
      "abstract    35472 non-null object\n",
      "concepts    35472 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 831.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"data.csv\")\n",
    "print(data_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that helps us extract the abstracts as plain text from the XMLLiterals returned by Virtuoso, removes all non-English words including stop words, and performs lemmatization on the filtered words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup_abstract(xmlstring):\n",
    "    #import ipdb; ipdb.set_trace()\n",
    "    xmlstring = xmlstring.replace('\"\"', '\"')\n",
    "    text = None\n",
    "    try: \n",
    "        tree = ET.ElementTree(ET.fromstring(xmlstring))\n",
    "        xpath_result = tree.findall(\".//description\")\n",
    "        text = xpath_result[0].text\n",
    "    except:\n",
    "        text = xmlstring\n",
    "    # remove stopwords and punctuation. lower case everything\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w.lower() for w in tokens if not w in stop_words and w.isalpha() and wordnet.synsets(w)]\n",
    "    # lemmatize\n",
    "    lemma = WordNetLemmatizer()\n",
    "    final_tokens = []\n",
    "    for word in tokens:\n",
    "        final_tokens.append(lemma.lemmatize(word))\n",
    "    ret = \" \".join(final_tokens)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data involves two steps:\n",
    "\n",
    "1\\. clean the abstracts \n",
    "\n",
    "2\\. transform the \";\" separated eurovoc codes in the concepts column into lists of eurovoc codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if CLEANUP_DATA:\n",
    "    data_df[\"cleaned_abstract\"] = data_df[\"abstract\"].apply(cleanup_abstract)\n",
    "    #data_df[\"concept_list\"] = data_df[\"concepts\"].apply(lambda x: [c for c in x.split(\";\")])\n",
    "    data_df[\"concept_list\"] = data_df[\"concepts\"].apply(lambda x: list({c[c.rfind(\"/\")+1:c.rfind(\"/\")+3] for c in x.split(\";\")}))\n",
    "    concepts_list = data_df[\"concepts\"].tolist()\n",
    "    data_df.drop([\"abstract\"], axis=1)\n",
    "    data_df.to_pickle(\"data_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line loads the cleaned up data directly from file instead of computing it everytime the notebook is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    at institute reference material measurement di...\n",
      "1    statistic focus describes preliminary result b...\n",
      "2    short document aim provide summary main issue ...\n",
      "3    briefing note intended provide european parlia...\n",
      "4    exterior de la sus la de para de en la en para...\n",
      "Name: cleaned_abstract, dtype: object\n",
      "0            [37, 40, 11, 53, 25, 34]\n",
      "1                        [42, 46, 63]\n",
      "2    [53, 52, 18, 43, 24, 19, 44, 17]\n",
      "3        [11, 52, 36, 10, 93, 13, 44]\n",
      "4             [14, 42, 27, 13, 31, 9]\n",
      "Name: concept_list, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_pickle(\"data_df.pkl\")\n",
    "print(data_df['cleaned_abstract'][:5])\n",
    "print(data_df['concept_list'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must convert the data set labels to numbers so that they can be processed by Keras. The appraoch is described in https://www.pyimagesearch.com/2018/05/07/multi-label-classification-with-keras/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = data_df[\"concept_list\"].tolist()\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(labels)\n",
    " \n",
    "# loop over each of the possible class labels and show them\n",
    "for (i, label) in enumerate(mlb.classes_):\n",
    "\tprint(\"{}. {}\".format(i + 1, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#labels2 = [[\"a\",\"b\",\"c\"],[\"c\",\"d\",\"e\"]]\n",
    "\n",
    "#mlb2 = MultiLabelBinarizer()\n",
    "#labels2 = mlb2.fit_transform(labels2)\n",
    "\n",
    "#print(list(mlb2.classes_))\n",
    "#print(mlb2.transform([[\"a\",\"b\"],[\"c\",\"d\",\"e\"]]))\n",
    "#print(mlb2.inverse_transform(mlb2.transform([[\"a\",\"b\"],[\"c\",\"d\",\"e\"]])))\n",
    "\n",
    "#y = data_df[\"concept_list\"].tolist()[0]\n",
    "#print(\"y = \", y)\n",
    "#print(\"len(y) = \", len(y))\n",
    "#print(\"mlb.transform([y]) = \", mlb.transform([y]))\n",
    "#print(\"len(mlb.transform([y])[0]) = \", len(mlb.transform([y])[0]))\n",
    "#print(\"mlb.transform([y])[0].tolist().count(1) = \", mlb.transform([y])[0].tolist().count(1))\n",
    "#print(\"mlb.inverse_transform(mlb.transform([y])) = \", mlb.inverse_transform(mlb.transform([y])))\n",
    "#print(\"len(mlb.classes_) = \", len(mlb.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small test to make sure that we are really getting multi-label vectors and not just one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Labels of the 2nd training example: \" + str(mlb.inverse_transform(np.array([labels[1]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to tranform the input into an array of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data_df[\"cleaned_abstract\"].tolist()\n",
    "\n",
    "#print(len(data[0]))\n",
    "#print(type(data[0]))\n",
    "#print(data[0])\n",
    "tokenizer = Tokenizer(nb_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "#print(len(sequences[0]))\n",
    "#print(sequences[0])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "#labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "print(num_validation_samples)\n",
    "trainX = data[:-num_validation_samples]\n",
    "trainY = labels[:-num_validation_samples]\n",
    "testX = data[-num_validation_samples:]\n",
    "testY = labels[-num_validation_samples:]\n",
    "\n",
    "print(data.shape)\n",
    "print(labels.shape)\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "print(testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-computed GloVe word embeddings from file and create an embeddings_index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(os.path.join('glove.6B', 'glove.6B.100d.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use word_index and embedding_index to compute the embedding_matrix. embedding_matrix is  a matrix storing the embedded_vector for each word in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    #else:\n",
    "    #    print(\"Not not in embedding index: \" + word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an Keras embedding_layer. Note that trainable=false, i.e., weights are not getting updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the 1D convolutional model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Dropout(0.25)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(labels.shape[1], activation='sigmoid')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model needs to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(trainX, trainY, validation_data=(testX, testY),\n",
    "          epochs=1, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data_df['cleaned_abstract'].tolist()[:2])\n",
    "ex = data_df['cleaned_abstract'][0]\n",
    "seq = tokenizer.texts_to_sequences(ex)\n",
    "seq = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(\"len(ex) = \", len(ex))\n",
    "print(\"seq.shape = \", seq.shape)\n",
    "print(\"Example: \" + str(ex) + \"\\n\")\n",
    "print(\"Sequence: \" + str(seq) + \"\\n\")\n",
    "prediction = model.predict(np.array(seq))[0]\n",
    "prediction[prediction>=0.25] = 1\n",
    "prediction[prediction<0.25] = 0\n",
    "print(\"Prediction: \" + str(prediction))\n",
    "print(type(prediction))\n",
    "print(prediction.shape)\n",
    "\n",
    "mlb.inverse_transform(np.array([prediction]))\n",
    "\n",
    "#proba = model.predict(np.array(seq))[0]\n",
    "#idxs = np.argsort(proba)[::-1][:2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
