{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepEurovoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this little experiment is to predict Eurovoc categories/fields of publications based on expression abstracts published by the PO. \n",
    "\n",
    "The model used in this notebook is based on the 1D CNN described in the Keras blog article https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html. It uses pre-trained GloVe word embeddings for text classification.\n",
    "\n",
    "The input data is retrieved from the public SPARQL endpoint of the PO which is available at http://publications.europa.eu/webapi/rdf/sparql. The following query does the job:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "prefix cdm: <http://publications.europa.eu/ontology/cdm#> \n",
    "select ?exp ?abstract (group_concat(?concept;separator=\";\") as ?concepts)  where \n",
    "{\n",
    "\t?exp cdm:expression_uses_language <http://publications.europa.eu/resource/authority/language/ENG>.\n",
    "\t?exp cdm:expression_abstract ?abstract.\n",
    "\t?exp cdm:expression_belongs_to_work/cdm:work_is_about_concept_eurovoc ?concept.\n",
    "} group by ?exp ?abstract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data has already been downloaded and is stored in data.csv. Before loading the data, the modules needed throughout this notebook are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ff76c7fc5651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0metree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mElementTree\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mET\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32md:\\programs\\python\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[1;31m# numpy compat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32md:\\programs\\python\\lib\\site-packages\\pandas\\compat\\numpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;31m# numpy versioning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0m_np_version\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0m_nlv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_np_version\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0m_np_version_under1p10\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_nlv\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'1.10'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as tfb\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of some global variables used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLEANUP_DATA = False # save time by loading a cleaned up version from disc\n",
    "MAX_NUM_WORDS = 20000 # max. size of vocabulary\n",
    "EMBEDDING_DIM = 100 # dimension of GloVe word embeddings\n",
    "MAX_SEQUENCE_LENGTH = 1000 # truncate examples after MAX_SEQUENCE_LENGTH words\n",
    "VALIDATION_SPLIT = 0.2 # ration for split of training data and test data\n",
    "NUM_EPOCHS = 1 # number of epochs the network is trained\n",
    "# those are the eurovoc fields we want to support\n",
    "EUROVOC_FIELDS = {\n",
    "\"4\":  \"POLITICS\",\n",
    "\"8\":  \"INTERNATIONAL RELATIONS\", \n",
    "\"10\": \"EUROPEAN UNION\",\n",
    "\"12\": \"LAW \",\n",
    "\"16\": \"ECONOMICS\",\n",
    "\"20\": \"TRADE\",\n",
    "\"24\": \"FINANCE\",\n",
    "\"28\": \"SOCIAL QUESTIONS\",\n",
    "\"32\": \"EDUCATION AND COMMUNICATIONS\",\n",
    "\"36\": \"SCIENCE\",\n",
    "\"40\": \"BUSINESS AND COMPETITION\",\n",
    "\"44\": \"EMPLOYMENT AND WORKING CONDITIONS\",\n",
    "\"48\": \"TRANSPORT\",\n",
    "\"52\": \"ENVIRONMENT\",\n",
    "\"56\": \"AGRICULTURE, FORESTRY AND FISHERIES\",\n",
    "\"60\": \"AGRI-FOODSTUFFS\",\n",
    "\"64\": \"PRODUCTION, TECHNOLOGY AND RESEARCH\",\n",
    "\"66\": \"ENERGY\",\n",
    "\"68\": \"INDUSTRY\",\n",
    "\"72\": \"GEOGRAPHY\",\n",
    "\"76\": \"INTERNATIONAL ORGANISATIONS\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and get some numbers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"data.csv\")\n",
    "print(data_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that will help us to extract the abstracts as plain text from the XMLLiterals returned by Virtuoso, to remove all non-English words including stop words, and to perform lemmatization on the filtered words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup_abstract(xmlstring):\n",
    "    #import ipdb; ipdb.set_trace()\n",
    "    xmlstring = xmlstring.replace('\"\"', '\"')\n",
    "    text = None\n",
    "    try: \n",
    "        tree = ET.ElementTree(ET.fromstring(xmlstring))\n",
    "        xpath_result = tree.findall(\".//description\")\n",
    "        text = xpath_result[0].text\n",
    "    except:\n",
    "        text = xmlstring\n",
    "    # remove stopwords and punctuation. lower case everything\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #tokens = word_tokenize(text)\n",
    "    #tokens = [w.lower() for w in tokens if not w in stop_words and w.isalpha() and wordnet.synsets(w)]\n",
    "    # lemmatize\n",
    "    #lemma = WordNetLemmatizer()\n",
    "    #final_tokens = []\n",
    "    #for word in tokens:\n",
    "    #    final_tokens.append(lemma.lemmatize(word))\n",
    "    #ret = \" \".join(final_tokens)\n",
    "    #return ret\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data involves two steps:\n",
    "\n",
    "1\\. processing of the abstracts \n",
    "\n",
    "2\\. transform the \";\" separated eurovoc codes in the concepts column into lists of eurovoc categories. Only consider to first two digits of the eurovoc codes. The total number of codes  in the data set is much too high (>5000) for classification. Therefore, the first two digits indicating the \"subject matter\" are extracted and filtered based on EUROVOC_FIELDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CLEANUP_DATA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a0077178e9fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mif\u001b[0m \u001b[0mCLEANUP_DATA\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdata_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"clean_abstract\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"abstract\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleanup_abstract\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"clean_concepts\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"concepts\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\";\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mEUROVOC_FIELDS\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_concepts'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'[]'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"abstract\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CLEANUP_DATA' is not defined"
     ]
    }
   ],
   "source": [
    "if CLEANUP_DATA:\n",
    "    data_df[\"clean_abstract\"] = data_df[\"abstract\"].apply(cleanup_abstract)\n",
    "    data_df[\"clean_concepts\"] = data_df[\"concepts\"].apply(lambda x: list({c[c.rfind(\"/\")+1:c.rfind(\"/\")+3] for c in x.split(\";\") if c[c.rfind(\"/\")+1:c.rfind(\"/\")+3] in EUROVOC_FIELDS}))\n",
    "    data_df = data_df[data_df.astype(str)['clean_concepts'] != '[]']\n",
    "    data_df.drop([\"abstract\"], axis=1)\n",
    "    data_df.drop([\"concepts\"], axis=1)\n",
    "    data_df.to_pickle(\"data_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line loads the cleaned up data directly from file instead of computing it everytime the notebook is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fd519dd69234>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data_df.pkl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_abstract'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_concepts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_pickle(\"data_df.pkl\")\n",
    "print(data_df['clean_abstract'][:5])\n",
    "print(data_df['clean_concepts'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must convert the data set labels to numbers so that they can be processed by Keras. The approach is described in https://www.pyimagesearch.com/2018/05/07/multi-label-classification-with-keras/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-836bbda95bb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"clean_concepts\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmlb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultiLabelBinarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;31m# loop over each of the possible class labels and show them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_df' is not defined"
     ]
    }
   ],
   "source": [
    "labels = data_df[\"clean_concepts\"].tolist()\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(labels)\n",
    "# loop over each of the possible class labels and show them\n",
    "for (i, label) in enumerate(mlb.classes_):\n",
    "\tprint(\"{}. {}\".format(i + 1, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small test to make sure that MultiLabelBinarizer is really generating multi-label vectors and not just one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-497a0642c9f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Labels of the 2nd training example: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mlb' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Labels of the 2nd training example: \" + str(mlb.inverse_transform(np.array([labels[1]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to tranform the input examples into an array of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bd4069dd679b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"clean_abstract\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_NUM_WORDS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mword_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_df' is not defined"
     ]
    }
   ],
   "source": [
    "data = data_df[\"clean_abstract\"].tolist()\n",
    "tokenizer = Tokenizer(nb_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "#num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "#print(num_validation_samples)\n",
    "#trainX = data[:-num_validation_samples]\n",
    "#trainY = labels[:-num_validation_samples]\n",
    "#testX = data[-num_validation_samples:]\n",
    "#testY = labels[-num_validation_samples:]\n",
    "\n",
    "#print(\"trainX.shape\", trainX.shape)\n",
    "#print(\"trainY.shape\", trainY.shape)\n",
    "#print(\"testX.shape\", testX.shape)\n",
    "#print(\"testY.shape\", testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-computed GloVe word embeddings from file and create an embeddings_index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(os.path.join('glove.6B', 'glove.6B.100d.txt'), 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use word_index and embedding_index to compute the embedding_matrix. embedding_matrix is  a matrix storing the embedded_vector for each word in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    #else:\n",
    "    #    print(\"Not not in embedding index: \" + word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an Keras embedding_layer. Note that trainable=false, i.e., weights are not getting updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the 1D convolutional model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig1 = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "fig1.savefig('tessstttyyy.png', dpi=100)\n",
    "\n",
    "\n",
    "x = np.random.randint(low=1, high=11, size=50)\n",
    "y = x + np.random.randint(1, 5, size=x.size)\n",
    "data = np.column_stack((x, y))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,\n",
    "                               figsize=(8, 4))\n",
    "\n",
    "ax1.scatter(x=x, y=y, marker='o', c='r', edgecolor='b')\n",
    "ax1.set_title('Scatter: $x$ versus $y$')\n",
    "ax1.set_xlabel('$x$')\n",
    "ax1.set_ylabel('$y$')\n",
    "ax2.hist(data, bins=np.arange(data.min(), data.max()),\n",
    "         label=('x', 'y'))\n",
    "ax2.legend(loc=(0.65, 0.8))\n",
    "ax2.set_title('Frequencies of $x$ and $y$')\n",
    "ax2.yaxis.tick_right()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25271 samples, validate on 6318 samples\n",
      "Epoch 1/1\n",
      "25271/25271 [==============================] - 239s 9ms/step - loss: 0.6158 - categorical_accuracy: 0.2381 - val_loss: 0.4412 - val_categorical_accuracy: 0.2657\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xt8FeW97/HPT+Si3G9uFWSDSqskJCHGgAUEhCJoAUUUUFS0ytHW6inbvUVLK6LtQbwhllNFC9u9paZUiqJV2NWNINuDECiiYCkIWGOocpebaPB3/pghrixWMiHJJAS/79drvbLmmWdmfs+slfVbz8ysZ8zdERERKcsJNR2AiIgc+5QsREQkkpKFiIhEUrIQEZFIShYiIhJJyUJERCIpWYiISCQli2pgZpvNrN8xEMdoMztkZnvN7HMzW2VmPwjnrTOzqxLqdjczT1G218xOTLHudDNbYGbbzOyofrxjZr3N7Otw3XvNrMDMZpvZ+Un1zMz+1czWm9kBM/u7mU0ys/plrLu+mc0I2/sPMxtbRt2qasOecH/ekFTHzWxfQjv3mtm/JczvaGZ5ZrY1jHe9mT1hZm3L2O7VZvZRuN4XzaxFGXWzzGyFme0P/2YlzDMze9DMtoePyWZmCfOnh2362sxGH+W+2Ry+XnvNbKeZ/cnMzkhRb0K4j3ITyu5J2FdfJLx/95rZmoTYbzez98P9UGBmfzCzzuH8fzezB5K21T7c1hHv5XD+w+H+32NmfzWz65LmJ7+WzyTNzzazxeG8T83sjqPZZ8ciJYtvn//n7o2AZsBvgdnhB8xioFdCvQuBv6Yoe9vdi1Ks9ytgNvDDCsZVGMbVGOgWbvstM+ubUGcqMAa4Lqw3ELgo3G5pJgAdgX8G+gD/ZmYDSqlbVW1oAvwUeNrMvptUJ9PdGyU8JgOY2dnAO0Ah0MXdmwDdgQ+BHqk2ZmZpwFPAtcA/AfuB/1tK3XrAS8BzQHPgWeClsByC/XoZkAlkAD8A/lfCKt4FfgSsLOe+SDYo3DenAZ8CTyTFZ2E7dgDXHy53918d3lfALYTv3/CRFlZ7HLgDuB1oAXwHeBG4tIKxAuwDBgFNw3geN7PvJdVJfC1vSmhLK2A+wWvTEjgb+K9KxHJscHc9Yn4Am4F+pcy7GdhA8E8yDzg9LDfgMeAzYDewGkgP510CrAX2AJ8Ad5YzjtHAkoTphoADOQT/qO8lzHs1rJ9cNj5iG2cHb6uj2j+9gYIU5b8G8sPnHYFDQG5SnTOAg8BFpaz7E6B/wvT9QF51tCF87a5MmHbg7FKWfw54+Si3+SvgdwnTZwFfAo1T1O0f7gtLKPs7MCB8/jYwJmHeD4GlKdazBBhdmfd/+P79W1KdC4EDwChgO1Av6v1b1vsiqc6/Aw8klbUPX48Ty9mGecC/lPO1/BXwn0ezj2rDQz2LGmRmFwH/B7iK4BvXR0BeOLs/wT/Qdwh6AcMJ/okg6BH8L3dvDKQD/52wzl1mlvKbaNK2TwRuAvYC64FFQJqZtTCzEwgSyO+BZgll3yPogVSXPwLZZtYQ6EvwYbwssYK7fwwsBb6fvLCZNQdOJ/hWfNi7QFpy3apkZieY2WCgFcEXgfLoB8w5yk2lkdA2d/+QIFl8p5S6qz38NAut5pt9UWJdxLSfzOxkgvfy0qRZ1wMvE7znIOjZlEfK90VVMrOTgPOBNUmzFoeHNv9oZu0TyrsBO8zsbTP7zMxeNrN2ccVXXZQsatY1wAx3X+nuB4G7gQvCN95XBIdaziH4NviBu28Jl/sK6GRmTdx9p7sXHxpw92buvqSMbXYzs13AP4CRwOXuvtvd/07wTbMnwaGI9e5+APifhLIGBIdKqkshQQ+rGcEH75ZS6m0J5ydrFP7dnVC2m2C/xuH0cN8eAOYCY939L0l1VoYJ/fDj4rC8FcFrAoCZ3RbO32tmT5eyvUaUbBuU3r6ousnzdwONEs9bVNKL4b75nCCxP3R4RphAriToJX0FvEDCoagILSn9fZHozsT9TpAoy+tJguS5IKGsF0Hv5ByC9+krCec/2hLEfwfQDtgEPH8U2zsmKVnUrNMJehMAuPtegt5DG3f/b4LDMNOAT8MTjE3CqlcQdOU/MrNFZnbBUWxzaZhQWrl7N3d/PWHeYoLezIXAW2HZkoSyd8KkVl3aEHT3dwHbCHpfqZwWzk+2N/zbJKGsCcHhuzgUunuzcBtTCc6nJMsO9//hx+EPoO0ktM/dfx2uawpQt5Tt7aVk26D09kXVTZ7fBNib1BOpjMvC9tQHbgMWmdmp4bzLgSKCw5wAs4CBZta6HOstsd/K8HDific4LxPJzB4i6L1flbgv3H2xu3/p7rsIkkIH4Nxw9gFgrrsvd/cvgPuA75lZ0/Js81ilZFGzCglOvAIQHm5pSXBsGXef6u7nERwO+A7wr2H5cncfApxCcCKvrBO8R+NwsujJN8nirYSy6jwEBcGHyEp330dwqO2MxCtlAMKraroBbyQv7O47Cb51ZiYUZ3Lk4YQqFSbUu4DOZnZZORd7Axh6lJtaQ0LbzOxMgg/jv5VSNyOpp5DBN/uixLqIaT+5+yF3/yPBeYbDh0uvJ+jZ/N3M/gH8gSBBjizHKt8A2ppZTlXHamb3EVxE0d/dP4+o7gS9YAh6LZ40j4T5tZKSRfWpa2YNEh4nAr8DbrDgksb6BCfG3nH3zWZ2vpl1NbO6BFdmfAEcMrN6ZnaNmTUNu+yfE/zjVYXFQBeCLvb/hGXvEXxr6kMZySK8fLEBUC+cbmAJl7SGly/+e1QA4XramNm9BOdU7gFw978RHA6YZWbdzKxOeDXQHOD1pB5Sov8AxptZczM7h+CCgpRxVFUbwni/BB4BflGe+gRXbfU0s0fNrE24vVZ88201lVnAIDPrGX7RmAj80d1T9SzeJHif3G7B5cS3heWHz3f9BzA23PenA/9Cwn4K33cNCD7wDr+XTwjn9bZyXmoc7uMhBFdkfRC2tS/BOYqs8JEJPEg5DkW5+3qCK8CeD+OoF8Y2wszGlSemUuK8G7ga+L67b0+alxb+z9Yxs0YEr/MnwAdhlZnA5WGdusDPCU7M76poPMeEmj7D/m14EFwN4kmPB8J5txBcHrkDeAVoG5b3JfiGspfgEMssgm9f9Qguy9tJkCiWAz0StrUX6FlKHKNJupokRZ1CghOhiWWvEpwnaVjGcu1TtHFzwvw3gJtLWbY38HUY+74whheAbkn1TiD4xr6BoKv/MTAZaFBGXPWBGeG++pTgPMLhee3CbbarojYkXw11cvjaDQqnPWzf3oTHlIT65xD0ErcRHB5aR3CJ6RlltO9qgnNN+wgujW2RMO814J6E6S7AinDfrSS4RPfwPAv35Y7wMZmSV069mWLf9A7nXUtwSXVZ7/8DYXv3AO8D14TzxgErUixzevieS496/4ax30HQE9pP8MH9eyAtnP/vHOXVUOG8g0mv1T3hvIvC12YfwRVvLwIdk5a/NYxjJ8GJ+1Jfw9rysLBhIrGx4Fr+d4EMD3pDtc7x0Ia4WPCDtD/4N+df5DikZCEiIpF0zkJERCIpWYiISCQlCxERiZRyxMXaqFWrVt6+ffuaDkNEpFZZsWLFNneP/AHkcZMs2rdvT35+fk2HISJSq5jZR9G1dBhKRETKQclCREQiKVmIiEik4+achYhUr6+++oqCggK++OKLmg5FyqFBgwa0bduWunVLG8S4bEoWIlIhBQUFNG7cmPbt21N1t72QOLg727dvp6CggA4dOlRoHToMJSIV8sUXX9CyZUslilrAzGjZsmWleoFKFiJSYUoUtUdlXyslCxERiaRkISK10vbt28nKyiIrK4tTTz2VNm3aFE9/+eWX5VrHDTfcwLp168qsM23aNGbNmlUVIddqOsEtIrVSy5YtWbVqFQATJkygUaNG3HnnnSXqHL5xzwknpP5ePHPmzMjt/PjHP658sMcB9SxE5LiyYcMG0tPTueWWW8jOzmbLli2MGTOGnJwc0tLSmDhxYnHdHj16sGrVKoqKimjWrBnjxo0jMzOTCy64gM8++wyA8ePHM2XKlOL648aNIzc3l+9+97u8/fbbAOzbt48rrriCzMxMRo4cSU5OTnEiS3Tvvfdy/vnnF8d3+H5Cf/vb37jooovIzMwkOzubzZs3A/CrX/2Kzp07k5mZyc9+9rM4d1sk9SxEpNLue3kNaws/r9J1djq9CfcOSqvQsmvXrmXmzJk8+eSTAEyaNIkWLVpQVFREnz59GDZsGJ06dSqxzO7du+nVqxeTJk1i7NixzJgxg3HjjryNt7uzbNky5s2bx8SJE5k/fz5PPPEEp556KnPmzOHdd98lOzs7ZVx33HEH9913H+7O1Vdfzfz58xk4cCAjR45kwoQJDBo0iC+++IKvv/6al19+mddee41ly5Zx0kknsWPHjgrti6qinoWIHHfOOusszj///OLp559/nuzsbLKzs/nggw9Yu3btEcucdNJJDBw4EIDzzjuv+Nt9sqFDhx5RZ8mSJYwYMQKAzMxM0tJSJ7k33niD3NxcMjMzWbRoEWvWrGHnzp1s27aNQYMGAcGP504++WRef/11brzxRk466SQAWrRocfQ7ogrF2rMwswHA40Ad4Bl3n5SizlXABIIbpL/r7leH5dcD48NqD7j7s3HGKiIVV9EeQFwaNmxY/Hz9+vU8/vjjLFu2jGbNmjFq1KiUvzeoV69e8fM6depQVFSUct3169c/ok55bk+9f/9+brvtNlauXEmbNm0YP358cRypLmt192Pq0uTYehZmVgeYBgwEOgEjzaxTUp2OwN1Ad3dPA/53WN4CuBfoCuQC95pZ87hiFZHj1+eff07jxo1p0qQJW7ZsYcGCBVW+jR49ejB79mwA3nvvvZQ9lwMHDnDCCSfQqlUr9uzZw5w5cwBo3rw5rVq14uWXXwaCHzvu37+f/v3789vf/pYDBw4AHNeHoXKBDe6+0d2/BPKAIUl1bgamuftOAHf/LCy/GPizu+8I5/0ZGBBjrCJynMrOzqZTp06kp6dz880307179yrfxk9+8hM++eQTMjIyeOSRR0hPT6dp06Yl6rRs2ZLrr7+e9PR0Lr/8crp27Vo8b9asWTzyyCNkZGTQo0cPtm7dyg9+8AMGDBhATk4OWVlZPPbYY1Ue99Gw8nSfKrRis2HAAHe/KZy+Fujq7rcl1HkR+BvQneBQ1QR3n29mdwIN3P2BsN7PgQPu/nDSNsYAYwDatWt33kcfleseHiJSBT744APOPffcmg7jmFBUVERRURENGjRg/fr19O/fn/Xr13PiicfWNUSpXjMzW+HuOVHLxtmSVAfbkjPTiUBHoDfQFnjLzNLLuSzuPh2YDpCTkxNP1hMRibB371769u1LUVER7s5TTz11zCWKyoqzNQXAGQnTbYHCFHWWuvtXwCYzW0eQPAoIEkjism/GFqmISCU0a9aMFStW1HQYsYrznMVyoKOZdTCzesAIYF5SnReBPgBm1gr4DrARWAD0N7Pm4Ynt/mGZiIjUgNh6Fu5eZGa3EXzI1wFmuPsaM5sI5Lv7PL5JCmuBQ8C/uvt2ADO7nyDhAEx095q9FEBE5Fss1oNq7v4q8GpS2S8SnjswNnwkLzsDmBFnfCIiUj76BbeIiERSshCRWql3795H/MBuypQp/OhHPypzuUaNGgFQWFjIsGHDSl13fn5+meuZMmUK+/fvL56+5JJL2LVrV3lCr5WULESkVho5ciR5eXklyvLy8hg5cmS5lj/99NN54YUXKrz95GTx6quv0qxZswqv71inZCEitdKwYcN45ZVXOHjwIACbN2+msLCQHj16FP/uITs7m86dO/PSSy8dsfzmzZtJT08HgqE4RowYQUZGBsOHDy8eYgPg1ltvLR7e/N577wVg6tSpFBYW0qdPH/r06QNA+/bt2bZtGwCPPvoo6enppKenFw9vvnnzZs4991xuvvlm0tLS6N+/f4ntHPbyyy/TtWtXunTpQr9+/fj000+B4LccN9xwA507dyYjI6N4uJD58+eTnZ1NZmYmffv2rZJ9m8rx9asREakZr42Df7xXtes8tTMMPGLs0WItW7YkNzeX+fPnM2TIEPLy8hg+fDhmRoMGDZg7dy5NmjRh27ZtdOvWjcGDB5c6MN9vfvMbTj75ZFavXs3q1atLDDH+y1/+khYtWnDo0CH69u3L6tWruf3223n00UdZuHAhrVq1KrGuFStWMHPmTN555x3cna5du9KrVy+aN2/O+vXref7553n66ae56qqrmDNnDqNGjSqxfI8ePVi6dClmxjPPPMPkyZN55JFHuP/++2natCnvvRfs5507d7J161ZuvvlmFi9eTIcOHWIdP0o9CxGptRIPRSUegnJ37rnnHjIyMujXrx+ffPJJ8Tf0VBYvXlz8oZ2RkUFGRkbxvNmzZ5OdnU2XLl1Ys2ZNykECEy1ZsoTLL7+chg0b0qhRI4YOHcpbb70FQIcOHcjKygJKHwa9oKCAiy++mM6dO/PQQw+xZs0aAF5//fUSd+1r3rw5S5cu5cILL6RDhw5AvMOYq2chIpVXRg8gTpdddhljx45l5cqVHDhwoLhHMGvWLLZu3cqKFSuoW7cu7du3TzkseaJUvY5Nmzbx8MMPs3z5cpo3b87o0aMj11PWeHuHhzeHYIjzVIehfvKTnzB27FgGDx7Mm2++yYQJE4rXmxxjdQ5jrp6FiNRajRo1onfv3tx4440lTmzv3r2bU045hbp167Jw4UKiBhm98MILmTVrFgDvv/8+q1evBoLhzRs2bEjTpk359NNPee2114qXady4MXv27Em5rhdffJH9+/ezb98+5s6dS8+ePcvdpt27d9OmTRsAnn32m9v49O/fn1//+tfF0zt37uSCCy5g0aJFbNq0CYh3GHMlCxGp1UaOHMm7775bfKc6gGuuuYb8/HxycnKYNWsW55xzTpnruPXWW9m7dy8ZGRlMnjyZ3NxcILjrXZcuXUhLS+PGG28sMbz5mDFjGDhwYPEJ7sOys7MZPXo0ubm5dO3alZtuuokuXbqUuz0TJkzgyiuvpGfPniXOh4wfP56dO3eSnp5OZmYmCxcupHXr1kyfPp2hQ4eSmZnJ8OHDy72doxXbEOXVLScnx6OuixaRqqMhymufygxRrp6FiIhEUrIQEZFIShYiIhJJyUJERCIpWYiISCQlCxERiaRkISK10vbt28nKyiIrK4tTTz2VNm3aFE9/+eWX5VrHDTfcwLp168qsM23atOIf7FVWjx49WLVqVZWsq7rFOtyHmQ0AHie4reoz7j4paf5o4CHgk7Do1+7+TDjvEHB4ZLK/u/vgOGMVkdqlZcuWxR+8EyZMoFGjRtx5550l6rg77s4JJ6T+Xjxz5szI7SSOx/RtFlvPwszqANOAgUAnYKSZdUpR9ffunhU+nkkoP5BQrkQhIuWyYcMG0tPTueWWW8jOzmbLli2MGTOmeJjxiRMnFtc9/E2/qKiIZs2aMW7cODIzM7ngggv47LPPgOCX04eHGe/Rowfjxo0jNzeX7373u7z99tsA7Nu3jyuuuILMzExGjhxJTk5OZA/iueeeo3PnzqSnp3PPPfcAUFRUxLXXXltcPnXqVAAee+wxOnXqRGZm5hGj1FaXOHsWucAGd98IYGZ5wBCg7CEbRaTWeXDZg/x1x1+rdJ3ntDiHu3LvqtCya9euZebMmTz55JMATJo0iRYtWlBUVESfPn0YNmwYnTqV/O66e/duevXqxaRJkxg7diwzZsxg3LhxR6zb3Vm2bBnz5s1j4sSJzJ8/nyeeeIJTTz2VOXPm8O6775YY4jyVgoICxo8fT35+Pk2bNqVfv3688sortG7dmm3bthUPQ374znuTJ0/mo48+ol69ejV2N744z1m0AT5OmC4Iy5JdYWarzewFMzsjobyBmeWb2VIzuyzGOEXkOHPWWWdx/vnnF08///zzZGdnk52dzQcffJBymPGTTjqJgQMHAqUPHw4wdOjQI+osWbKkeGyqzMxM0tLSyozvnXfe4aKLLqJVq1bUrVuXq6++msWLF3P22Wezbt067rjjDhYsWEDTpk0BSEtLY9SoUcyaNYu6dese1b6oKnH2LFKNm5s8ENXLwPPuftDMbgGeBS4K57Vz90IzOxP4bzN7z90/LLEBszHAGIB27dpVbfQiUm4V7QHEpWHDhsXP169fz+OPP86yZcto1qwZo0aNSjnMeL169Yqf16lTh6KiopTrPjzMeGKdox1jr7T6LVu2ZPXq1bz22mtMnTqVOXPmMH36dBYsWMCiRYt46aWXeOCBB3j//fepU6fOUW2zsuLsWRQAiT2FtkBhYgV33+7uB8PJp4HzEuYVhn83Am8CRwzb6O7T3T3H3XNat25dtdGLyHHh888/p3HjxjRp0oQtW7awYMGCKt9Gjx49mD17NgDvvfde5A2SunXrxsKFC9m+fTtFRUXk5eXRq1cvtm7dirtz5ZVXct9997Fy5UoOHTpEQUEBF110EQ899BBbt24tce/v6hJnz2I50NHMOhBc7TQCuDqxgpmd5u5bwsnBwAdheXNgf9jjaAV0BybHGKuIHKeys7Pp1KkT6enpnHnmmSWGGa8qP/nJT7juuuvIyMggOzub9PT04kNIqbRt25aJEyfSu3dv3J1BgwZx6aWXsnLlSn74wx8W39TowQcfpKioiKuvvpo9e/bw9ddfc9ddd9G4ceMqb0OUWIcoN7NLgCkEl87OcPdfmtlEIN/d55nZ/yFIEkXADuBWd/+rmX0PeAr4mqD3M8Xdf1vWtjREuUj10hDl3ygqKqKoqIgGDRqwfv16+vfvz/r16znxxGPrZqSVGaI81pa4+6vAq0llv0h4fjdwd4rl3gY6xxmbiEhV2bt3L3379qWoqAh356mnnjrmEkVlHV+tERGpAc2aNWPFihU1HUasNNyHiIhEUrIQEZFIShYiIhJJyUJERCIpWYhIrdS7d+8jfmA3ZcoUfvSjH5W5XKNGjQAoLCxk2LBhpa476lL8KVOmlPhx3CWXXFIl4zZNmDCBhx9+uNLrqWpKFiJSK40cOZK8vLwSZXl5eYwcObJcy59++um88MILFd5+crJ49dVXadasWYXXd6xTshCRWmnYsGG88sorHDwYjBi0efNmCgsL6dGjR/HvHrKzs+ncuTMvvfTSEctv3ryZ9PR0AA4cOMCIESPIyMhg+PDhHDhwoLjerbfeWjy8+b333gvA1KlTKSwspE+fPvTp0weA9u3bs23bNgAeffRR0tPTSU9PLx7efPPmzZx77rncfPPNpKWl0b9//xLbSWXVqlV069aNjIwMLr/8cnbu3Fm8/U6dOpGRkVE8gOGiRYuKb/7UpUsX9uzZU+F9m4p+ZyEilfaPX/2Kgx9U7RDl9c89h1PD+zyk0rJlS3Jzc5k/fz5DhgwhLy+P4cOHY2Y0aNCAuXPn0qRJE7Zt20a3bt0YPHgwZqnGN4Xf/OY3nHzyyaxevZrVq1eXGGL8l7/8JS1atODQoUP07duX1atXc/vtt/Poo4+ycOFCWrVqVWJdK1asYObMmbzzzju4O127dqVXr140b96c9evX8/zzz/P0009z1VVXMWfOnDLvT3HdddfxxBNP0KtXL37xi19w3333MWXKFCZNmsSmTZuoX79+8aGvhx9+mGnTptG9e3f27t1LgwYNjmZ3R1LPQkRqrcRDUYmHoNyde+65h4yMDPr168cnn3zCp59+Wup6Fi9eXPyhnZGRQUZGRvG82bNnk52dTZcuXVizZk3kIIFLlizh8ssvp2HDhjRq1IihQ4fy1ltvAdChQweysrKAsodBh+D+Grt27aJXr14AXH/99SxevLg4xmuuuYbnnnuu+Jfi3bt3Z+zYsUydOpVdu3ZV+S/I1bMQkUorqwcQp8suu4yxY8eycuVKDhw4UNwjmDVrFlu3bmXFihXUrVuX9u3bpxyWPFGqXsemTZt4+OGHWb58Oc2bN2f06NGR6ylrvL3Dw5tDMMR51GGo0vzpT39i8eLFzJs3j/vvv581a9Ywbtw4Lr30Ul599VW6devG66+/zjnnnFOh9aeinoWI1FqNGjWid+/e3HjjjSVObO/evZtTTjmFunXrsnDhQj766KMy13PhhRcya9YsAN5//31Wr14NBMObN2zYkKZNm/Lpp5/y2muvFS/TuHHjlOcFLrzwQl588UX279/Pvn37mDt3Lj179jzqtjVt2pTmzZsX90r+8z//k169evH111/z8ccf06dPHyZPnsyuXbvYu3cvH374IZ07d+auu+4iJyeHv/61ag8LqmchIrXayJEjGTp0aIkro6655hoGDRpETk4OWVlZkd+wb731Vm644QYyMjLIysoiNzcXCO5616VLF9LS0o4Y3nzMmDEMHDiQ0047jYULFxaXZ2dnM3r06OJ13HTTTXTp0qXMQ06lefbZZ7nlllvYv38/Z555JjNnzuTQoUOMGjWK3bt34+789Kc/pVmzZvz85z9n4cKF1KlTh06dOhXf9a+qxDpEeXXSEOUi1UtDlNc+lRmiXIehREQkkpKFiIhEUrIQkQo7Xg5jfxtU9rVSshCRCmnQoAHbt29XwqgF3J3t27dX6od6sV4NZWYDgMcJ7sH9jLtPSpo/GngI+CQs+rW7PxPOux4YH5Y/4O7PxhmriBydtm3bUlBQwNatW2s6FCmHBg0a0LZt2wovH1uyMLM6wDTg+0ABsNzM5rl78s8ff+/utyUt2wK4F8gBHFgRLrszrnhF5OjUrVuXDh061HQYUk3iPAyVC2xw943u/iWQBwwp57IXA3929x1hgvgzMCCmOEVEJEKcyaIN8HHCdEFYluwKM1ttZi+Y2RlHs6yZjTGzfDPLV1dYRCQ+cSaLVMM7Jp8Jexlo7+4ZwOvA4fMS5VkWd5/u7jnuntO6detKBSsiIqWLM1kUAGckTLcFChMruPt2dz8YTj4NnFfeZUVEpPrEmSyWAx3NrIOZ1QNGAPMSK5jZaQmTg4EPwucLgP5m1tzMmgP9wzIREakBsV0N5e5FZnYbwYd8HWCGu68xs4lAvrvPA243s8FAEbADGB0uu8PM7idIOAAT3X1HXLGKiEjZNJCgiMi3mAYSFBGRKqNkISImDKDTAAAQIUlEQVQikZQsREQkkpKFiIhEUrIQEZFIShYiIhJJyUJERCIpWYiISCQlCxERiaRkISIikZQsREQkkpKFiIhEUrIQEZFIShYiIhJJyUJERCIpWYiISCQlCxERiRRrsjCzAWa2zsw2mNm4MuoNMzM3s5xwur2ZHTCzVeHjyTjjFBGRssV2D24zqwNMA74PFADLzWyeu69NqtcYuB14J2kVH7p7VlzxiYhI+cXZs8gFNrj7Rnf/EsgDhqSodz8wGfgixlhERKQS4kwWbYCPE6YLwrJiZtYFOMPdX0mxfAcz+4uZLTKznqk2YGZjzCzfzPK3bt1aZYGLiEhJcSYLS1HmxTPNTgAeA/4lRb0tQDt37wKMBX5nZk2OWJn7dHfPcfec1q1bV1HYIiKSLM5kUQCckTDdFihMmG4MpANvmtlmoBswz8xy3P2gu28HcPcVwIfAd2KMVUREyhBnslgOdDSzDmZWDxgBzDs80913u3srd2/v7u2BpcBgd883s9bhCXLM7EygI7AxxlhFRKQMsV0N5e5FZnYbsACoA8xw9zVmNhHId/d5ZSx+ITDRzIqAQ8At7r4jrlhFRKRs5u7RtWqBnJwcz8/Pr+kwRERqFTNb4e45UfXKdRjKzO4wsyYW+K2ZrTSz/pUPU0REaoPynrO40d0/B/oDrYEbgEmxRSUiIseU8iaLw5fBXgLMdPd3SX1prIiIHIfKmyxWmNl/ESSLBeEQHV/HF5aIiBxLyns11A+BLGCju+83sxYEh6JERORboLw9iwuAde6+y8xGAeOB3fGFJSIix5LyJovfAPvNLBP4N+Aj4D9ii0pERI4p5U0WRR78IGMI8Li7P04wXIeIiHwLlPecxR4zuxu4FugZDsVRN76wRETkWFLensVw4CDB7y3+QTDU+EOxRSUiIseUciWLMEHMApqa2Q+AL9xd5yxERL4lyjvcx1XAMuBK4CrgHTMbFmdgIiJy7CjvOYufAee7+2cAZtYaeB14Ia7ARETk2FHecxYnHE4Uoe1HsayIiNRy5e1ZzDezBcDz4fRw4NV4QhIRkWNNuZKFu/+rmV0BdCcYQHC6u8+NNTIRETlmlPtOee4+B5gTYywiInKMKvO8g5ntMbPPUzz2mNnnUSs3swFmts7MNpjZuDLqDTMzN7OchLK7w+XWmdnFR9csERGpSmX2LNy9wkN6hL/yngZ8HygAlpvZPHdfm1SvMXA78E5CWSdgBJAGnA68bmbfcfdDFY1HREQqLs4rmnKBDe6+0d2/BPIIxpZKdj8wGfgioWwIkOfuB919E7AhXJ+IiNSAOJNFG+DjhOmCsKyYmXUBznD3V4522XD5MWaWb2b5W7durZqoRUTkCHEmi1S3XfXimWYnAI8B/3K0yxYXuE939xx3z2ndunWFAxURkbKV+2qoCigAzkiYbgsUJkw3BtKBN80M4FRgnpkNLseyIiJSjeLsWSwHOppZBzOrR3DCet7hme6+291buXt7d28PLAUGu3t+WG+EmdU3sw5AR4KxqUREpAbE1rNw9yIzuw1YANQBZrj7GjObCOS7+7wyll1jZrOBtUAR8GNdCSUiUnMsuAFe7ZeTk+P5+fk1HYaISK1iZivcPSeqngYDFBGRSEoWIiISSclCREQiKVmIiEgkJQsREYmkZCEiIpGULEREJJKShYiIRFKyEBGRSEoWIiISSclCREQiKVmIiEgkJQsREYmkZCEiIpGULEREJJKShYiIRFKyEBGRSLEmCzMbYGbrzGyDmY1LMf8WM3vPzFaZ2RIz6xSWtzezA2H5KjN7Ms44RUSkbLHdg9vM6gDTgO8DBcByM5vn7msTqv3O3Z8M6w8GHgUGhPM+dPesuOITEZHyi7NnkQtscPeN7v4lkAcMSazg7p8nTDYEjo8bgouIHGfiTBZtgI8TpgvCshLM7Mdm9iEwGbg9YVYHM/uLmS0ys56pNmBmY8ws38zyt27dWpWxi4hIgjiThaUoO6Ln4O7T3P0s4C5gfFi8BWjn7l2AscDvzKxJimWnu3uOu+e0bt26CkMXEZFEcSaLAuCMhOm2QGEZ9fOAywDc/aC7bw+frwA+BL4TU5wiIhIhzmSxHOhoZh3MrB4wApiXWMHMOiZMXgqsD8tbhyfIMbMzgY7AxhhjFRGRMsR2NZS7F5nZbcACoA4ww93XmNlEIN/d5wG3mVk/4CtgJ3B9uPiFwEQzKwIOAbe4+464YhURkbKZ+/FxAVJOTo7n5+fXdBgiIrWKma1w95yoevoFt4iIRFKyEBGRSEoWIiISSclCREQiKVmIiEgkJQsREYmkZCEiIpGULEREJJKShYiIRFKyEBGRSEoWIiISSclCREQiKVmIiEgkJQsREYmkZCEiIpGULEREJJKShYiIRIo1WZjZADNbZ2YbzGxcivm3mNl7ZrbKzJaYWaeEeXeHy60zs4vjjFNERMoWW7IwszrANGAg0AkYmZgMQr9z987ungVMBh4Nl+0EjADSgAHA/w3XJyIiNSDOnkUusMHdN7r7l0AeMCSxgrt/njDZEDh8Q/AhQJ67H3T3TcCGcH0iIlIDToxx3W2AjxOmC4CuyZXM7MfAWKAecFHCskuTlm2TYtkxwBiAdu3aVUnQIiJypDh7FpaizI8ocJ/m7mcBdwHjj3LZ6e6e4+45rVu3rlSwIiJSujiTRQFwRsJ0W6CwjPp5wGUVXFZERGIUZ7JYDnQ0sw5mVo/ghPW8xApm1jFh8lJgffh8HjDCzOqbWQegI7AsxlhFRKQMsZ2zcPciM7sNWADUAWa4+xozmwjku/s84DYz6wd8BewErg+XXWNms4G1QBHwY3c/FFesIiJSNnM/4lRArZSTk+P5+fk1HYaISK1iZivcPSeqnn7BLSIikZQsREQkkpKFiIhEUrIQEZFIShYiIhJJyUJERCIpWYiISCQlCxERiaRkISIikZQsREQkkpKFiIhEUrIQEZFIShYiIhJJyUJERCIpWYiISCQlCxERiaRkISIikZQsREQkUqzJwswGmNk6M9tgZuNSzB9rZmvNbLWZvWFm/5ww75CZrQof8+KMU0REynZiXCs2szrANOD7QAGw3MzmufvahGp/AXLcfb+Z3QpMBoaH8w64e1Zc8YmISPnF2bPIBTa4+0Z3/xLIA4YkVnD3he6+P5xcCrSNMR4REamgOJNFG+DjhOmCsKw0PwReS5huYGb5ZrbUzC5LtYCZjQnr5G/durXyEYuISEqxHYYCLEWZp6xoNgrIAXolFLdz90IzOxP4bzN7z90/LLEy9+nAdICcnJyU6xYRkcqLs2dRAJyRMN0WKEyuZGb9gJ8Bg9394OFydy8M/24E3gS6xBiriIiUIc5ksRzoaGYdzKweMAIocVWTmXUBniJIFJ8llDc3s/rh81ZAdyDxxLiIiFSj2A5DuXuRmd0GLADqADPcfY2ZTQTy3X0e8BDQCPiDmQH83d0HA+cCT5nZ1wQJbVLSVVQiIlKNzP34ONSfk5Pj+fn5NR2GiEitYmYr3D0nqp5+wS0iIpGULEREJJKShYiIRFKyEBGRSEoWIiISSclCREQiKVmIiEgkJQsREYmkZCEiIpGULEREJNJxM9yHmW0FPqrpOCqgFbCtpoOoZmrzt4PaXDv8s7u3jqp03CSL2srM8sszLsvxRG3+dlCbjy86DCUiIpGULEREJJKSRc2bXtMB1AC1+dtBbT6O6JyFiIhEUs9CREQiKVmIiEgkJYtqYGYtzOzPZrY+/Nu8lHrXh3XWm9n1KebPM7P344+48irTZjM72cz+ZGZ/NbM1ZjapeqMvPzMbYGbrzGyDmY1LMb++mf0+nP+OmbVPmHd3WL7OzC6uzrgro6JtNrPvm9kKM3sv/HtRdcdeUZV5ncP57cxsr5ndWV0xVzl31yPmBzAZGBc+Hwc8mKJOC2Bj+Ld5+Lx5wvyhwO+A92u6PXG3GTgZ6BPWqQe8BQys6TaliL8O8CFwZhjnu0CnpDo/Ap4Mn48Afh8+7xTWrw90CNdTp6bbFHObuwCnh8/TgU9quj1xtzlh/hzgD8CdNd2eij7Us6geQ4Bnw+fPApelqHMx8Gd33+HuO4E/AwMAzKwRMBZ4oBpirSoVbrO773f3hQDu/iWwEmhbDTEfrVxgg7tvDOPMI2h3osT98ALQ18wsLM9z94PuvgnYEK7vWFfhNrv7X9y9MCxfAzQws/rVEnXlVOZ1xswuI/gitKaa4o2FkkX1+Cd33wIQ/j0lRZ02wMcJ0wVhGcD9wCPA/jiDrGKVbTMAZtYMGAS8EVOclREZf2Iddy8CdgMty7nssagybU50BfAXdz8YU5xVqcJtNrOGwF3AfdUQZ6xOrOkAjhdm9jpwaopZPyvvKlKUuZllAWe7+0+Tj4PWtLjanLD+E4HnganuvvHoI4xdmfFH1CnPsseiyrQ5mGmWBjwI9K/CuOJUmTbfBzzm7nvDjkatpWRRRdy9X2nzzOxTMzvN3beY2WnAZymqFQC9E6bbAm8CFwDnmdlmgtfrFDN70917U8NibPNh04H17j6lCsKNQwFwRsJ0W6CwlDoFYfJrCuwo57LHosq0GTNrC8wFrnP3D+MPt0pUps1dgWFmNhloBnxtZl+4+6/jD7uK1fRJk2/DA3iIkid7J6eo0wLYRHCCt3n4vEVSnfbUnhPclWozwfmZOcAJNd2WMtp4IsGx6A58c+IzLanOjyl54nN2+DyNkie4N1I7TnBXps3NwvpX1HQ7qqvNSXUmUItPcNd4AN+GB8Hx2jeA9eHfwx+IOcAzCfVuJDjRuQG4IcV6alOyqHCbCb65OfABsCp83FTTbSqlnZcAfyO4WuZnYdlEYHD4vAHBVTAbgGXAmQnL/ixcbh3H4NVeVd1mYDywL+E1XQWcUtPtift1TlhHrU4WGu5DREQi6WooERGJpGQhIiKRlCxERCSSkoWIiERSshARkUhKFiLHADPrbWav1HQcIqVRshARkUhKFiJHwcxGmdkyM1tlZk+ZWZ3wPgWPmNlKM3vDzFqHdbPMbKmZrTazuYfv6WFmZ5vZ62b2brjMWeHqG5nZC+F9PGYdHrVU5FigZCFSTmZ2LjAc6O7uWcAh4BqgIbDS3bOBRcC94SL/Adzl7hnAewnls4Bp7p4JfA/YEpZ3Af43wb0uzgS6x94okXLSQIIi5dcXOA9YHn7pP4lggMSvgd+HdZ4D/mhmTYFm7r4oLH8W+IOZNQbauPtcAHf/AiBc3zJ3LwinVxEM77Ik/maJRFOyECk/A55197tLFJr9PKleWWPolHVoKfHeDofQ/6ccQ3QYSqT83iAYbvoUKL7P+D8T/B8NC+tcDSxx993ATjPrGZZfCyxy988JhrG+LFxHfTM7uVpbIVIB+uYiUk7uvtbMxgP/ZWYnAF8RDE29D0gzsxUEd0gbHi5yPfBkmAw2AjeE5dcCT5nZxHAdV1ZjM0QqRKPOilSSme1190Y1HYdInHQYSkREIqlnISIikdSzEBGRSEoWIiISSclCREQiKVmIiEgkJQsREYn0/wFmGPKAryyq7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2f5ec908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25271 samples, validate on 6318 samples\n",
      "Epoch 1/1\n",
      "23040/25271 [==========================>...] - ETA: 19s - loss: 2.1298 - categorical_accuracy: 0.2054"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-040383352383>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 history = model.fit(data, labels, validation_split=0.2,\n\u001b[0;32m---> 57\u001b[0;31m                           epochs=NUM_EPOCHS, batch_size=BATCH_SIZE)\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_%(pos_weight)s_%(dropout)s_%(regularization)s_%(batch_size)s.h5\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for POS_WEIGHT in [1, 5, 10]:\n",
    "    for DROPOUT in [0.1, 0.25, 0.5]:\n",
    "        for REGULARIZATION in [0.001, 0.01, 0.1]:\n",
    "            for BATCH_SIZE in [256]:\n",
    "                \n",
    "                params ={\"pos_weight\": str(POS_WEIGHT), \n",
    "                          \"dropout\": str(DROPOUT), \n",
    "                          \"regularization\": str(REGULARIZATION), \n",
    "                          \"batch_size\": str(BATCH_SIZE)}\n",
    "                \n",
    "                def weighted_binary_crossentropy(target, output):\n",
    "                    \"\"\"\n",
    "                    Weighted binary crossentropy between an output tensor \n",
    "                    and a target tensor. POS_WEIGHT is used as a multiplier \n",
    "                    for the positive targets.\n",
    "\n",
    "                    Combination of the following functions:\n",
    "                    * keras.losses.binary_crossentropy\n",
    "                    * keras.backend.tensorflow_backend.binary_crossentropy\n",
    "                    * tf.nn.weighted_cross_entropy_with_logits\n",
    "                    \"\"\"\n",
    "                    # transform back to logits\n",
    "                    _epsilon = tfb._to_tensor(tfb.epsilon(), output.dtype.base_dtype)\n",
    "                    output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
    "                    output = tf.log(output / (1 - output))\n",
    "                    # compute weighted loss\n",
    "                    loss = tf.nn.weighted_cross_entropy_with_logits(targets=target,\n",
    "                                                                    logits=output,\n",
    "                                                                    pos_weight=POS_WEIGHT)\n",
    "                    return tf.reduce_mean(loss, axis=-1)\n",
    "\n",
    "\n",
    "                sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "                embedded_sequences = embedding_layer(sequence_input)\n",
    "                x = Conv1D(128, 5, activation='relu', kernel_regularizer=regularizers.l2(REGULARIZATION))(embedded_sequences)\n",
    "                x = MaxPooling1D(5)(x)\n",
    "                x = Conv1D(128, 5, activation='relu', kernel_regularizer=regularizers.l2(REGULARIZATION))(x)\n",
    "                x = MaxPooling1D(5)(x)\n",
    "                x = Conv1D(128, 5, activation='relu', kernel_regularizer=regularizers.l2(REGULARIZATION))(x)\n",
    "                x = MaxPooling1D(35)(x)  # global max pooling\n",
    "                x = Flatten()(x)\n",
    "                x = Dropout(DROPOUT)(x)\n",
    "                x = Dense(128, activation='relu',kernel_regularizer=regularizers.l2(REGULARIZATION))(x)\n",
    "                x = Dropout(DROPOUT)(x)\n",
    "                preds = Dense(labels.shape[1], activation='sigmoid', kernel_regularizer=regularizers.l2(REGULARIZATION))(x)\n",
    "\n",
    "\n",
    "                import keras.losses\n",
    "                keras.losses.weighted_binary_crossentropy = weighted_binary_crossentropy\n",
    "\n",
    "                model = Model(sequence_input, preds)\n",
    "                model.compile(loss='weighted_binary_crossentropy',\n",
    "                              optimizer='adam',\n",
    "                              metrics=['categorical_accuracy'])\n",
    "\n",
    "                history = model.fit(data, labels, validation_split=0.2,\n",
    "                          epochs=NUM_EPOCHS, batch_size=BATCH_SIZE)\n",
    "                \n",
    "                model.save(\"model_%(pos_weight)s_%(dropout)s_%(regularization)s_%(batch_size)s.h5\"%params)\n",
    "                \n",
    "                acc = history.history['categorical_accuracy']\n",
    "                val_acc = history.history['val_categorical_accuracy']\n",
    "                loss = history.history['loss']\n",
    "                val_loss = history.history['val_loss']\n",
    "\n",
    "                epochs = range(len(acc))\n",
    "\n",
    "                plt.plot(epochs, acc, label='Training acc')\n",
    "                plt.plot(epochs, val_acc, label='Validation acc')\n",
    "                plt.ylabel('accuracy')\n",
    "                plt.xlabel('epoch')\n",
    "                plt.title('Accuracy: PW %(pos_weight)s, DO %(dropout)s, REG %(regularization)s, BATCH %(batch_size)s'% params)\n",
    "                plt.legend()\n",
    "                \n",
    "                plt.figure()\n",
    "\n",
    "                plt.plot(epochs, loss, label='Training loss')\n",
    "                plt.plot(epochs, val_loss, label='Validation loss')\n",
    "                plt.ylabel('loss')\n",
    "                plt.xlabel('epoch')\n",
    "                plt.title('Loss: PW %(pos_weight)s, DO %(dropout)s, REG %(regularization)s, BATCH %(batch_size)s'%params)\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "                #f.savefig(\"model_%(pos_weight)s_%(dropout)s_%(regularization)s_%(batch_size)s.png\"%params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model needs to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to disc!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict using an example from the training set. Later we should also create a dev set for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
