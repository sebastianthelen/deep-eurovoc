{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepEurovoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this little experiment is to predict Eurovoc categories of publications based on expression abstracts published by the PO. \n",
    "\n",
    "The model used in this notebook is based on the 1D CNN described in the Keras blog article https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html. It uses pre-trained GloVe word embeddings for text classification.\n",
    "\n",
    "The input data is retrieved from the public SPARQL endpoint of the PO which is available at http://publications.europa.eu/webapi/rdf/sparql. The following query does the job:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "prefix cdm: <http://publications.europa.eu/ontology/cdm#> \n",
    "select ?exp ?abstract (group_concat(?concept;separator=\";\") as ?concepts)  where \n",
    "{\n",
    "\t?exp cdm:expression_uses_language <http://publications.europa.eu/resource/authority/language/ENG>.\n",
    "\t?exp cdm:expression_abstract ?abstract.\n",
    "\t?exp cdm:expression_belongs_to_work/cdm:work_is_about_concept_eurovoc ?concept.\n",
    "} group by ?exp ?abstract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data has already been downloaded and is stored in data.csv. Before loading the data, the modules needed throughout this notebook are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#import keras\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "#from keras.utils import to_categorical\n",
    "#from keras.layers import Dense, Input, GlobalMaxPooling1D, Flatten\n",
    "#from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "#from keras.models import Model\n",
    "#from keras.initializers import Constant\n",
    "#from keras.layers import Dropout\n",
    "\n",
    "#from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of some global variables used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CLEANUP_DATA = True # save time by loading a cleaned up version from disc\n",
    "MAX_NUM_WORDS = 20000 # max. size of vocabulary\n",
    "EMBEDDING_DIM = 100 # dimension of GloVe word embeddings\n",
    "MAX_SEQUENCE_LENGTH = 1000 # truncate examples after MAX_SEQUENCE_LENGTH words\n",
    "VALIDATION_SPLIT = 0.2 # ration for split of training data and test data\n",
    "NUM_EPOCHS = 30 # number of epochs the network is trained\n",
    "EUROVOC_FIELDS = {\n",
    "\"4\":  \"POLITICS\",\n",
    "\"8\":  \"INTERNATIONAL RELATIONS\", \n",
    "\"10\": \"EUROPEAN UNION\",\n",
    "\"12\": \"LAW \",\n",
    "\"16\": \"ECONOMICS\",\n",
    "\"20\": \"TRADE\",\n",
    "\"24\": \"FINANCE\",\n",
    "\"28\": \"SOCIAL QUESTIONS\",\n",
    "\"32\": \"EDUCATION AND COMMUNICATIONS\",\n",
    "\"36\": \"SCIENCE\",\n",
    "\"40\": \"BUSINESS AND COMPETITION\",\n",
    "\"44\": \"EMPLOYMENT AND WORKING CONDITIONS\",\n",
    "\"48\": \"TRANSPORT\",\n",
    "\"52\": \"ENVIRONMENT\",\n",
    "\"56\": \"AGRICULTURE, FORESTRY AND FISHERIES\",\n",
    "\"60\": \"AGRI-FOODSTUFFS\",\n",
    "\"64\": \"PRODUCTION, TECHNOLOGY AND RESEARCH\",\n",
    "\"66\": \"ENERGY\",\n",
    "\"68\": \"INDUSTRY\",\n",
    "\"72\": \"GEOGRAPHY\",\n",
    "\"76\": \"INTERNATIONAL ORGANISATIONS\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and get some numbers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35472 entries, 0 to 35471\n",
      "Data columns (total 3 columns):\n",
      "exp         35472 non-null object\n",
      "abstract    35472 non-null object\n",
      "concepts    35472 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 831.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"data.csv\")\n",
    "print(data_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that will help us to extract the abstracts as plain text from the XMLLiterals returned by Virtuoso, to remove all non-English words including stop words, and to perform lemmatization on the filtered words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup_abstract(xmlstring):\n",
    "    #import ipdb; ipdb.set_trace()\n",
    "    xmlstring = xmlstring.replace('\"\"', '\"')\n",
    "    text = None\n",
    "    try: \n",
    "        tree = ET.ElementTree(ET.fromstring(xmlstring))\n",
    "        xpath_result = tree.findall(\".//description\")\n",
    "        text = xpath_result[0].text\n",
    "    except:\n",
    "        text = xmlstring\n",
    "    # remove stopwords and punctuation. lower case everything\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w.lower() for w in tokens if not w in stop_words and w.isalpha() and wordnet.synsets(w)]\n",
    "    # lemmatize\n",
    "    lemma = WordNetLemmatizer()\n",
    "    final_tokens = []\n",
    "    for word in tokens:\n",
    "        final_tokens.append(lemma.lemmatize(word))\n",
    "    ret = \" \".join(final_tokens)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data involves two steps:\n",
    "\n",
    "1\\. processing of the abstracts \n",
    "\n",
    "2\\. transform the \";\" separated eurovoc codes in the concepts column into lists of eurovoc categories. Only consider to first two digits of the eurovoc codes. The total number of codes  in the data set is much too high (>5000) for classification. Therefore, the first two digits indication the \"subject matter\" are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if CLEANUP_DATA:\n",
    "    data_df[\"clean_abstract\"] = data_df[\"abstract\"].apply(cleanup_abstract)\n",
    "    data_df[\"clean_concepts\"] = data_df[\"concepts\"].apply(lambda x: list({c[c.rfind(\"/\")+1:c.rfind(\"/\")+3] for c in x.split(\";\") if c[c.rfind(\"/\")+1:c.rfind(\"/\")+3] in EUROVOC_FIELDS}))\n",
    "    #concepts_list = data_df[\"concepts\"].tolist()\n",
    "    data_df.drop([\"abstract\"], axis=1)\n",
    "    data_df.to_pickle(\"data_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line loads the cleaned up data directly from file instead of computing it everytime the notebook is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    at institute reference material measurement di...\n",
      "1    statistic focus describes preliminary result b...\n",
      "2    short document aim provide summary main issue ...\n",
      "3    briefing note intended provide european parlia...\n",
      "4    exterior de la sus la de para de en la en para...\n",
      "Name: clean_abstract, dtype: object\n",
      "0                [40]\n",
      "1                  []\n",
      "2        [52, 24, 44]\n",
      "3    [36, 52, 44, 10]\n",
      "4                  []\n",
      "Name: clean_concepts, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_pickle(\"data_df.pkl\")\n",
    "print(data_df['clean_abstract'][:5])\n",
    "print(data_df['clean_concepts'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 31589 entries, 0 to 35471\n",
      "Data columns (total 5 columns):\n",
      "exp               31589 non-null object\n",
      "abstract          31589 non-null object\n",
      "concepts          31589 non-null object\n",
      "clean_abstract    31589 non-null object\n",
      "clean_concepts    31589 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 1.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data_df = data_df[data_df.astype(str)['clean_concepts'] != '[]']\n",
    "print(data_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must convert the data set labels to numbers so that they can be processed by Keras. The approach is described in https://www.pyimagesearch.com/2018/05/07/multi-label-classification-with-keras/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = data_df[\"clean_concepts\"].tolist()\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(labels)\n",
    "# loop over each of the possible class labels and show them\n",
    "for (i, label) in enumerate(mlb.classes_):\n",
    "\tprint(\"{}. {}\".format(i + 1, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small test to make sure that MultiLabelBinarizer is really generating multi-label vectors and not just one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Labels of the 2nd training example: \" + str(mlb.inverse_transform(np.array([labels[1]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to tranform the input examples into an array of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data_df[\"clean_abstract\"].tolist()\n",
    "\n",
    "#print(len(data[0]))\n",
    "#print(type(data[0]))\n",
    "#print(data[0])\n",
    "tokenizer = Tokenizer(nb_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "#print(len(sequences[0]))\n",
    "#print(sequences[0])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "#labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "print(num_validation_samples)\n",
    "trainX = data[:-num_validation_samples]\n",
    "trainY = labels[:-num_validation_samples]\n",
    "testX = data[-num_validation_samples:]\n",
    "testY = labels[-num_validation_samples:]\n",
    "\n",
    "print(\"trainX.shape\", trainX.shape)\n",
    "print(\"trainY.shape\", trainY.shape)\n",
    "print(\"testX.shape\", testX.shape)\n",
    "print(\"testY.shape\", testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-computed GloVe word embeddings from file and create an embeddings_index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(os.path.join('glove.6B', 'glove.6B.100d.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use word_index and embedding_index to compute the embedding_matrix. embedding_matrix is  a matrix storing the embedded_vector for each word in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    #else:\n",
    "    #    print(\"Not not in embedding index: \" + word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an Keras embedding_layer. Note that trainable=false, i.e., weights are not getting updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the 1D convolutional model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Dropout(0.25)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(labels.shape[1], activation='sigmoid')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model needs to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(trainX, trainY, validation_data=(testX, testY),\n",
    "          epochs=NUM_EPOCHS, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to disc!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc = history.history['categorical_accuracy']\n",
    "val_acc = history.history['val_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, label='Training acc')\n",
    "plt.plot(epochs, val_acc, label='Validation acc')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, label='Training loss')\n",
    "plt.plot(epochs, val_loss, label='Validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict using an example from the training set. Later we should also create a dev set for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ex = data_df['clean_abstract'][0]\n",
    "seq = tokenizer.texts_to_sequences(ex)\n",
    "seq = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(\"ex = \", ex)\n",
    "print(\"len(ex) = \", len(ex))\n",
    "print(\"seq.shape = \", seq.shape)\n",
    "print(\"Example: \" + str(ex) + \"\\n\")\n",
    "print(\"Sequence: \" + str(seq) + \"\\n\")\n",
    "prediction = model.predict(np.array(seq))[0]\n",
    "prediction[prediction>=0.5] = 1\n",
    "prediction[prediction<0.5] = 0\n",
    "print(\"Prediction: \" + str(prediction))\n",
    "print(\"type(prediction) = \", type(prediction))\n",
    "print(\"prediction.shape = \", prediction.shape)\n",
    "\n",
    "mlb.inverse_transform(np.array([prediction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
